use std::{collections::HashMap, path::PathBuf, fmt::Debug};
use markdown_it::parser::extset::MarkdownItExt;
use tokio::task::JoinSet;


#[derive(Debug)]
enum ScraperState {
    Fulfilling,
    Scraping,
}

type ResponseHandler = Box<dyn FnOnce(String, String) -> String + Send + Sync + 'static>;

//type ResponseHandler = dyn FnOnce(&reqwest::Response) -> dyn Future<Output = String>;

pub struct Scraper {
    cache_path: PathBuf,
    state: ScraperState,
    cached_responses: HashMap<String, HashMap<String, String>>,
    pending_requests: Vec<(reqwest::Request, (String, ResponseHandler))>,
    client: reqwest::Client,
}

impl MarkdownItExt for Scraper {}

impl Debug for Scraper {
    fn fmt(&self, _f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        // Overriding Debug for scraper, because I don't know how to 
        // implement Debug for ResponseHandler.
        Ok(())
    }
}

impl Scraper {
    pub fn new_fulfiller(path: PathBuf) -> Self {
        Self {
            cache_path: path.clone(),
            state: ScraperState::Fulfilling,
            cached_responses: serde_yaml::from_str(
                std::fs::read_to_string(path.clone())
                .expect(format!("Failed to read scraper cache {:?}", path).as_str())
                .as_str()
            ).expect("Failed to deserialize scraper cache. Consider regenerating the cache."),
            pending_requests: vec![],
            client: reqwest::Client::new(),
        }
    }

    pub fn new_scraper(path: PathBuf) -> Self {
        Self {
            state: ScraperState::Scraping,
            cached_responses: HashMap::new(),
            cache_path: path,
            pending_requests: vec![],
            client: reqwest::Client::new(),
        }
    }

    pub fn get(
        &mut self, 
        attribute: &str, 
        req_handler: impl FnOnce(&reqwest::Client) -> reqwest::Request, 
        handler: impl FnOnce(String, String) -> String + Send + Sync + 'static
    ) -> String {
        let attribute = attribute.to_string();
        let request = req_handler(&self.client);

        match self.state {
            ScraperState::Scraping => {
                self.pending_requests.push((request, (attribute, Box::new(handler))));
                return "".into()
            }
            ScraperState::Fulfilling => {
                let url = request.url().to_string();
                let rebuild = "Consider using the --build-cache flag to regenerate the cache.";
                return self.cached_responses
                    .get(&url)
                    .expect(format!("Scraper has not scraped {:?}. {}", url, rebuild).as_str())
                    .get(&attribute)
                    .expect(format!("Scraper has scraped {:?} but has no entry for attribute \"{}\". {}", request, attribute, rebuild).as_str())
                    .clone()
            }
        }
    }

    pub async fn into_cache(self) {
        let mut join_set = JoinSet::new();
        for (request, (attribute, response_handler)) in self.pending_requests.into_iter() {
            join_set.spawn(async move {
                let url = request.url().to_string();
                let resp = reqwest::Client::new()
                    .execute(request).await
                    .expect("");

                let status = resp.status();

                if !(status.is_success() || status.is_redirection()) {
                    eprintln!("    {} for {}", status, url);
                }

                (
                    url.clone(),
                    attribute,
                    response_handler(
                        url,
                        resp.text().await
                        .expect("")
                    )
                )
            });
        }

        let mut cache: HashMap<String, HashMap<String, String>> = HashMap::new();
        while let Some(result) = join_set.join_next().await {
            let (url, attribute, parsed_response) = result.expect("Error getting HTTP result");
            cache
                .entry(url).or_default()
                .entry(attribute).or_insert(parsed_response);
        }

        let mut text = String::from(r#"
# This cache was autogenerated by the --build-cache flag.
# It's a lookup table for web scraped data which cannot be reproducibly fetched 
# at deploy time.

# 'Mentioning' new URL's or DOI's will panic until --build-cache is re-ran.

"#);

        text.push_str(
            serde_yaml::to_string(&cache)
            .expect("Failed to serialize scraper to YAML")
            .as_str()
        );

        std::fs::write(self.cache_path, text).expect("Error scraper to disk");
    }
}
