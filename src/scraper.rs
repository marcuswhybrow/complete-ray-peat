use std::{collections::HashMap, path::Path, fmt::Debug};
use markdown_it::parser::extset::MarkdownItExt;
use tokio::task::JoinSet;


#[derive(Debug)]
enum ScraperState {
    Fulfilling,
    Scraping,
}

type ResponseHandler = Box<dyn FnOnce(String, String) -> String + Send + Sync + 'static>;

pub enum ScraperKind {
    Fulfiller,
    Scraper,
}

pub struct Scraper<'a> {
    cache_path: &'a Path,
    state: ScraperState,
    cached_responses: HashMap<String, HashMap<String, String>>,
    pending_requests: Vec<(reqwest::Request, (String, ResponseHandler))>,
    client: reqwest::Client,
}

impl MarkdownItExt for Scraper<'static> {}

impl<'a> Debug for Scraper<'a> {
    fn fmt(&self, _f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        // Overriding Debug for scraper, because I don't know how to 
        // implement Debug for ResponseHandler.
        Ok(())
    }
}

impl<'a> Scraper<'a> {
    pub fn new(path: &'a Path, kind: ScraperKind) -> Self {
        match kind {
            ScraperKind::Fulfiller => {
                Self {
                    cache_path: path.clone(),
                    state: ScraperState::Fulfilling,
                    cached_responses: serde_yaml::from_str(
                        std::fs::read_to_string(path.clone())
                        .expect(format!("Failed to read scraper cache {:?}", path).as_str())
                        .as_str()
                    ).expect("Failed to deserialize scraper cache. Consider regenerating the cache."),
                    pending_requests: vec![],
                    client: reqwest::Client::new(),
                }
            },
            ScraperKind::Scraper => {
                Self {
                    state: ScraperState::Scraping,
                    cached_responses: HashMap::new(),
                    cache_path: path,
                    pending_requests: vec![],
                    client: reqwest::Client::new(),
                }

            },
        }
    }

    pub fn get(
        &mut self, 
        attribute: &str, 
        req_handler: impl FnOnce(&reqwest::Client) -> reqwest::Request, 
        handler: impl FnOnce(String, String) -> String + Send + Sync + 'static
    ) -> String {
        let attribute = attribute.to_string();
        let request = req_handler(&self.client);

        match self.state {
            ScraperState::Scraping => {
                self.pending_requests.push((request, (attribute, Box::new(handler))));
                return "".into()
            }
            ScraperState::Fulfilling => {
                let url = request.url().to_string();
                let rebuild = "Consider using the --build-cache flag to regenerate the cache.";
                return self.cached_responses
                    .get(&url)
                    .expect(format!("Scraper has not scraped {:?}. {}", url, rebuild).as_str())
                    .get(&attribute)
                    .expect(format!("Scraper has scraped {:?} but has no entry for attribute \"{}\". {}", request, attribute, rebuild).as_str())
                    .clone()
            }
        }
    }

    pub async fn into_cache(self) {
        let mut join_set = JoinSet::new();
        for (request, (attribute, response_handler)) in self.pending_requests.into_iter() {
            join_set.spawn(async move {
                let url = request.url().to_string();

                match reqwest::Client::new().execute(request).await {
                    Ok(resp) => {
                        let status = resp.status();

                        if !(status.is_success() || status.is_redirection()) {
                            eprintln!("    {status} for {url} seeking \"{attribute}\"");
                        }

                        let body = resp.text().await
                            .expect(format!("Failed to extract body from HTTP response from {}", url).as_str());

                        (url.clone(), attribute, response_handler(url.clone(), body))
                    },
                    Err(e) => {
                        // TODO returning URL on error does always make sense
                        // Consider adding an error_handler closure argument.
                        eprintln!("    Bad request for {url} seeking \"{attribute}\". Returning \"{url}\" instead. The error was:");
                        eprintln!("      {e}");
                        (url.clone(), attribute, url.clone())
                    }
                }
            });
        }

        let mut cache: HashMap<String, HashMap<String, String>> = HashMap::new();
        while let Some(result) = join_set.join_next().await {
            let (url, attribute, parsed_response) = result.expect("Error getting HTTP result");
            cache
                .entry(url).or_default()
                .entry(attribute).or_insert(parsed_response);
        }

        let mut text = String::from(r#"
# This cache was autogenerated by the --build-cache flag.
# It's a lookup table for web scraped data which cannot be reproducibly fetched 
# at deploy time.

# 'Mentioning' new URL's or DOI's will panic until --build-cache is re-ran.

"#);

        text.push_str(
            serde_yaml::to_string(&cache)
            .expect("Failed to serialize scraper to YAML")
            .as_str()
        );

        std::fs::write(self.cache_path, text).expect("Error scraper to disk");
    }
}
